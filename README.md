# Social-Choice-and-Alignment

## Overviews
- [Social Choice for AI Alignment: Dealing with Diverse Human Feedback](https://arxiv.org/abs/2404.10271)
- [AI Alignment and Social Choice: Fundamental Limitations and Policy Implications](https://arxiv.org/abs/2310.16048)
- [Mapping Social Choice Theory to RLHF](https://arxiv.org/pdf/2404.13038)
- [Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2307.15217#page=12.5)
  - In particular sections 3.2.1 and 4.1 

## Constitutional AI
- [Collective Constitutional AI: Aligning a Language Model with Public Input](https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input)
- [Generative Social Choice](https://arxiv.org/pdf/2309.01291)

## Reward Modelling
- [Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF](https://arxiv.org/abs/2312.08358)
- [Crowd-PrefRL: Preference-Based Reward Learning from Crowds](https://arxiv.org/abs/2401.10941)
- [Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons](https://arxiv.org/abs/2301.11270)
- [Soft Condorcet Optimization](https://drive.google.com/file/d/1D_N_zb-2sHz54v_piikTTNvW52SCswHw/view)
- [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/abs/2402.01306)
- [Preference Ranking Optimization for Human Alignment](https://arxiv.org/abs/2306.17492)
- [RLHF and IIA: Perverse Incentives](https://arxiv.org/abs/2312.01057)
- [A Minimaximalist Approach to Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2401.04056)
- [Fine-Grained Human Feedback Gives Better Rewards for Language Model Training](https://arxiv.org/abs/2306.01693)


## Preference Aggregation
- [Moral Machine or Tyranny of the Majority?](https://arxiv.org/pdf/2305.17319)
- [https://aclanthology.org/2023.findings-acl.658/](https://aclanthology.org/2023.findings-acl.658/)
- [Batch Active Preference-Based Learning of Reward Functions](https://proceedings.mlr.press/v87/biyik18a/biyik18a.pdf)
- [On Releasing Annotator-Level Labels and Information in Datasets](https://arxiv.org/abs/2110.05699)

## Datasets
### Language
- [The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models](https://arxiv.org/abs/2404.16019)
- [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)
### Other

## Blog Posts
- [My lab's small AI safety agenda, Jobst Heitzig](https://forum.effectivealtruism.org/posts/ZWjDkENuFohPShTyc/my-lab-s-small-ai-safety-agenda)
- [A proposal for importing societyâ€™s values, Jan Leike](https://aligned.substack.com/p/a-proposal-for-importing-societys-values)
- [Stop "reinventing" everything to solve alignment, Nathan Lambert](https://www.interconnects.ai/p/reinventing-llm-alignment)
